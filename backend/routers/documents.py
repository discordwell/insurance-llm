import json
import base64

from fastapi import APIRouter, HTTPException
from config import MOCK_MODE, OPENAI_MODEL
from services.llm import get_client, clean_llm_response
from services.mock.extract import mock_extract
from schemas.common import DocumentInput, ExtractedPolicy, OCRInput, ClassifyInput, ClassifyResult
from data.supported_doc_types import SUPPORTED_DOC_TYPES
from prompts.extraction import EXTRACTION_PROMPT
from prompts.classify import CLASSIFY_PROMPT, OCR_PROMPT

router = APIRouter(prefix="/api", tags=["documents"])


@router.post("/extract", response_model=ExtractedPolicy)
async def extract_document(doc: DocumentInput):
    """Extract structured data from insurance document text"""
    try:
        # Use mock extraction in mock mode
        if MOCK_MODE:
            extracted = mock_extract(doc.text)
            return ExtractedPolicy(**extracted)

        prompt = EXTRACTION_PROMPT.replace("<<DOCUMENT>>", doc.text)
        response = get_client().chat.completions.create(
            model=OPENAI_MODEL,
            max_completion_tokens=4096,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )

        response_text = response.choices[0].message.content
        # Clean up potential markdown formatting
        if response_text.startswith("```"):
            response_text = response_text.split("```")[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]
        response_text = response_text.strip()

        extracted = json.loads(response_text)
        return ExtractedPolicy(**extracted)

    except json.JSONDecodeError as e:
        raise HTTPException(status_code=500, detail=f"Failed to parse LLM response: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/compare")
async def compare_quotes(quotes: list[DocumentInput]):
    """Compare multiple insurance quotes"""
    if len(quotes) < 2:
        raise HTTPException(status_code=400, detail="Need at least 2 quotes to compare")

    # Extract each quote
    extracted_quotes = []
    for quote in quotes:
        extracted = await extract_document(quote)
        extracted_quotes.append(extracted)

    # Generate comparison
    comparison_prompt = f"""Compare these {len(extracted_quotes)} insurance quotes and provide a recommendation.

Quotes:
{[q.model_dump() for q in extracted_quotes]}

Provide a JSON response with:
- recommendation: Which quote is best and why (string)
- comparison_table: Array of objects comparing key metrics
- pros_cons: Object with quote index as key, containing pros and cons arrays
- cost_analysis: Premium comparison and value assessment
- risk_assessment: Which provides better risk coverage

Return ONLY valid JSON."""

    try:
        response = get_client().chat.completions.create(
            model=OPENAI_MODEL,
            max_completion_tokens=4096,
            messages=[{"role": "user", "content": comparison_prompt}]
        )

        response_text = response.choices[0].message.content
        if response_text.startswith("```"):
            response_text = response_text.split("```")[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]

        return json.loads(response_text.strip())

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/generate-proposal")
async def generate_proposal(extracted: ExtractedPolicy):
    """Generate a polished client-ready proposal from extracted data"""

    # Mock proposal generation
    if MOCK_MODE:
        coverages_text = "\n".join([f"- **{c.type}**: {c.limit}" for c in extracted.coverages]) if extracted.coverages else "- No coverages specified"
        exclusions_text = "\n".join([f"- {e}" for e in extracted.exclusions]) if extracted.exclusions else "- None noted"

        proposal = f"""# Insurance Coverage Summary

## Policy Overview
- **Insured**: {extracted.insured_name or 'Not specified'}
- **Policy Number**: {extracted.policy_number or 'Not specified'}
- **Carrier**: {extracted.carrier or 'Not specified'}
- **Premium**: {extracted.total_premium or 'Not specified'}

## Coverage Details
{coverages_text}

## Important Exclusions
{exclusions_text}

## Risk Assessment
Coverage adequacy score: **{extracted.risk_score or 'N/A'}/100**

## Recommendations
1. Review all exclusions carefully with your broker
2. Consider additional coverage for any identified gaps
3. Verify all policy dates and deadlines

---
*Generated by Insurance.exe - Pixel Perfect Coverage Analysis*
"""
        return {"proposal": proposal}

    proposal_prompt = f"""Create a professional insurance proposal summary for a client based on this extracted policy data:

{extracted.model_dump()}

Write a clear, client-friendly proposal that:
1. Summarizes key coverages in plain English
2. Highlights important dates and deadlines
3. Notes any gaps or concerns
4. Provides actionable recommendations

Format as markdown with clear sections. Keep it concise but comprehensive."""

    try:
        response = get_client().chat.completions.create(
            model=OPENAI_MODEL,
            max_completion_tokens=2048,
            messages=[{"role": "user", "content": proposal_prompt}]
        )

        return {"proposal": response.choices[0].message.content}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/ocr")
async def ocr_document(input: OCRInput):
    """Extract text from PDF or image using OpenAI Vision API"""
    try:
        # Mock mode - return placeholder text
        if MOCK_MODE:
            return {
                "text": f"[Mock OCR result for {input.file_name}]\n\nSample extracted text would appear here.\nUpload a real document with OPENAI_API_KEY configured."
            }

        client = get_client()

        # Decode base64 file data
        file_bytes = base64.b64decode(input.file_data)

        # Handle PDFs by converting to images first
        if input.file_type == 'application/pdf':
            import fitz  # PyMuPDF
            import io

            # Open PDF from bytes
            pdf_doc = fitz.open(stream=file_bytes, filetype="pdf")

            all_text = []
            # Process each page (limit to first 5 pages for performance)
            for page_num in range(min(len(pdf_doc), 5)):
                page = pdf_doc[page_num]
                # Render page to image at 150 DPI for good quality
                pix = page.get_pixmap(dpi=150)
                img_bytes = pix.tobytes("png")
                img_base64 = base64.b64encode(img_bytes).decode('utf-8')

                # Create data URL for image
                data_url = f"data:image/png;base64,{img_base64}"

                # Call Vision API for this page
                response = client.chat.completions.create(
                    model="gpt-5.2",
                    max_completion_tokens=4096,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": OCR_PROMPT
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": data_url
                                    }
                                }
                            ]
                        }
                    ]
                )

                page_text = response.choices[0].message.content
                if len(pdf_doc) > 1:
                    all_text.append(f"--- Page {page_num + 1} ---\n{page_text}")
                else:
                    all_text.append(page_text)

            pdf_doc.close()
            return {"text": "\n\n".join(all_text)}

        # Handle images directly
        elif input.file_type.startswith('image/'):
            media_type = input.file_type
            data_url = f"data:{media_type};base64,{input.file_data}"

            response = client.chat.completions.create(
                model="gpt-5.2",
                max_completion_tokens=4096,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": OCR_PROMPT
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": data_url
                                }
                            }
                        ]
                    }
                ]
            )

            return {"text": response.choices[0].message.content}

        else:
            raise HTTPException(status_code=400, detail=f"Unsupported file type: {input.file_type}")

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"OCR failed: {str(e)}")


@router.post("/classify", response_model=ClassifyResult)
async def classify_document(input: ClassifyInput):
    """Classify document type using cheap/fast model"""
    try:
        # Mock mode
        if MOCK_MODE:
            text_lower = input.text.lower()
            if 'certificate' in text_lower and ('insurance' in text_lower or 'liability' in text_lower):
                doc_type = "coi"
            elif 'lease' in text_lower or ('landlord' in text_lower and 'tenant' in text_lower):
                doc_type = "lease"
            elif 'gym' in text_lower or 'fitness' in text_lower or ('membership' in text_lower and ('cancel' in text_lower or 'dues' in text_lower)):
                doc_type = "gym_contract"
            elif 'timeshare' in text_lower or 'vacation ownership' in text_lower or ('resort' in text_lower and 'maintenance fee' in text_lower):
                doc_type = "timeshare_contract"
            elif 'influencer' in text_lower or 'brand deal' in text_lower or ('content' in text_lower and 'usage rights' in text_lower) or 'sponsorship' in text_lower:
                doc_type = "influencer_contract"
            elif 'independent contractor' in text_lower or 'freelance' in text_lower or ('contractor' in text_lower and 'deliverables' in text_lower):
                doc_type = "freelancer_contract"
            elif 'employment' in text_lower or 'non-compete' in text_lower or ('employee' in text_lower and ('arbitration' in text_lower or 'at-will' in text_lower)):
                doc_type = "employment_contract"
            elif 'policy' in text_lower and 'premium' in text_lower:
                doc_type = "insurance_policy"
            elif 'agreement' in text_lower or 'contract' in text_lower:
                doc_type = "contract"
            else:
                doc_type = "unknown"

            doc_info = SUPPORTED_DOC_TYPES[doc_type]
            return ClassifyResult(
                document_type=doc_type,
                confidence=0.85,
                description=doc_info["name"],
                supported=doc_info["supported"]
            )

        client = get_client()

        # Use cheap model for classification - just need first ~2000 chars
        sample_text = input.text[:2000]

        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Cheap and fast
            max_completion_tokens=150,
            messages=[
                {"role": "system", "content": CLASSIFY_PROMPT},
                {"role": "user", "content": f"Classify this document:\n\n{sample_text}"}
            ]
        )

        response_text = response.choices[0].message.content.strip()

        # Parse JSON response
        if response_text.startswith("```"):
            response_text = response_text.split("```")[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]
        response_text = response_text.strip()

        result = json.loads(response_text)
        doc_type = result.get("type", "unknown")

        # Validate doc_type
        if doc_type not in SUPPORTED_DOC_TYPES:
            doc_type = "unknown"

        doc_info = SUPPORTED_DOC_TYPES[doc_type]

        return ClassifyResult(
            document_type=doc_type,
            confidence=result.get("confidence", 0.5),
            description=doc_info["name"],
            supported=doc_info["supported"]
        )

    except json.JSONDecodeError:
        return ClassifyResult(
            document_type="unknown",
            confidence=0.0,
            description="Could not classify document",
            supported=False
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Classification failed: {str(e)}")
